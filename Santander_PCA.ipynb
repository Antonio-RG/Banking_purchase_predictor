{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, decomposition, model_selection\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput \n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'input_data'\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "data_prefix = 'santander_project/data'\n",
    "seed=0\n",
    "n_components=200\n",
    "\n",
    "# Reading in training dataset\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'), index_col='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Reduction\n",
    "Here i choose to explore various kernels for a standard PCA approach, before ultimately selecting one for use. I also separately conducting a sparse PCA model using SageMaker. The computational cost of the sparse pca is such that i simply saved the output for potential use in modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Kernel-based PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, comparing performance of various PCA kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling features to means\n",
    "std_scale = preprocessing.StandardScaler().fit(train_df.iloc[:, 1:])\n",
    "train_df_scaled = std_scale.transform(train_df.iloc[:, 1:])\n",
    "\n",
    "# Applying Box-Cox transform to target\n",
    "train_df['target'] = stats.boxcox(train_df['target'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through kernels and plotting component correlations to target\n",
    "kernels=['linear', 'poly', 'rbf', 'sigmoid', 'cosine']\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "kernel_outputs = {}\n",
    "\n",
    "for i, kernel in enumerate(kernels):\n",
    "    kpca_sm = decomposition.KernelPCA(n_components=n_components, kernel=kernel)\n",
    "    kpca_sm.fit(train_df_scaled)\n",
    "    train_df_kpca = pd.concat([train_df['target'], pd.DataFrame(kpca_sm.transform(train_df_scaled), columns=['c{}'.format(num+1) for num in range(n_components)], index=train_df.index)], axis=1)\n",
    "    correlation_kpca = train_df_kpca.corr(method='spearman')['target'][1:]\n",
    "    \n",
    "    #Saving results in dictionary for post-evaluation use\n",
    "    kernel_outputs[kernel] = {}\n",
    "    kernel_outputs[kernel]['coeffs'] = correlation_kpca\n",
    "    kernel_outputs[kernel]['data']= train_df_kpca\n",
    "    kernel_outputs[kernel]['transformer'] = kpca_sm\n",
    "    \n",
    "    #Plotting results\n",
    "    ax = fig.add_subplot(3, 2, i+1)\n",
    "    ax.scatter(range(len(correlation_kpca)), np.sort(correlation_kpca))\n",
    "    ax.set_title('Component/Target Correlation ({} Kernel)'.format(kernel))\n",
    "    ax.set_xlabel('Components')\n",
    "    ax.set_ylabel('Correlation')\n",
    "    ax.set_ylim(-.3, .3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial kernel appears to have the highest magnitude of correlations to the target. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conducting Standard PCA Reduction and Saving Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA using a polynomial model appeared to perform the best. Using a gridsearch to optimize the paramaters, assuming this will be ultimately fitted to a gradient boosting regression model. Selecting components meeting correlation threshold and saving to a dataframe for regression modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pca__degree': 3, 'pca__gamma': 10}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimizing parameters\n",
    "\n",
    "poly_kpca = decomposition.KernelPCA(n_components=n_components, kernel='poly')\n",
    "GBRegressor = GradientBoostingRegressor()\n",
    "\n",
    "X = train_df_scaled\n",
    "y = train_df['target'] \n",
    "\n",
    "pipe = Pipeline(steps=[('pca', poly_kpca), ('grb_regressor', GBRegressor)])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__gamma': [1, 10, 100, 1000],\n",
    "    'pca__degree': [3, 4, 5],\n",
    "}\n",
    "\n",
    "pca_search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "pca_search.fit(X, y)\n",
    "\n",
    "pca_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_kpca = pca_search.best_estimator_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting PCA algorithm to data\n",
    "\n",
    "optimized_kpca.fit(train_df_scaled)\n",
    "train_df_optimized = pd.concat([train_df['target'], pd.DataFrame(optimized_kpca.transform(train_df_scaled), columns=['c{}'.format(num+1) for num in range(n_components)], index=train_df.index)], axis=1)\n",
    "correlation_optimized = train_df_optimized.corr(method='spearman')['target'][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing components based on correlation threshold\n",
    "\n",
    "pca_threshold = .05\n",
    "best_components = correlation_optimized[abs(correlation_optimized) >= pca_threshold].sort_values(ascending=False)\n",
    "pca_cols = [['target'] + list(best_components.index)][0]\n",
    "train_df_final_pca = train_df_optimized.loc[:, pca_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataframe of best components to local drive\n",
    "train_df_final_pca.to_csv(os.path.join(data_dir, 'train_pca.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming test data using fitted standard scaler and PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming and saving test data using selected PCA kernel\n",
    "chunksize = 5000 \n",
    "for chunk in pd.read_csv(os.path.join(data_dir, 'test.csv'), index_col='ID', chunksize=chunksize):\n",
    "        scaled_chunk = std_scale.transform(chunk.values)\n",
    "        transformed_chunk = optimized_kpca.transform(scaled_chunk)\n",
    "        transformed_chunk_df = pd.DataFrame(transformed_chunk, columns=['c{}'.format(num+1) for num in range(n_components)], index=chunk.index).loc[:, list(best_components.index)]\n",
    "        transformed_chunk_df.to_csv(os.path.join(data_dir, 'test_pca.csv'), header=False, index=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-278383315865/santander_project/data/test_pca.csv'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uploading transformed files to sagemaker for later use\n",
    "sagemaker_session.upload_data(path=os.path.join(data_dir, 'train_pca.csv'), bucket=bucket, key_prefix=data_prefix)\n",
    "sagemaker_session.upload_data(path=os.path.join(data_dir, 'test_pca.csv'), bucket=bucket, key_prefix=data_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conducting Sparse PCA Reduction and Saving Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performance of sparse PCA on the dataset. Because of the computational cost, i am executing the transformation using the SageMaker API and saving the output for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting data from S3\n",
    "train_path = 's3://{}/{}/train.csv'.format(bucket, data_prefix)\n",
    "test_path = 's3://{}/{}/test.csv'.format(bucket, data_prefix)\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.c4.xlarge',\n",
    "                                     instance_count=1)\n",
    "\n",
    "#Run imputation processing script\n",
    "sklearn_processor.run(code='processing_scripts/sparse_PCA.py',\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=train_path,\n",
    "                        destination='/opt/ml/processing/input/train'), ProcessingInput(\n",
    "                        source=test_path,\n",
    "                        destination='/opt/ml/processing/input/test'),],\n",
    "                      outputs=[ProcessingOutput(output_name='train_pca',\n",
    "                                                source='/opt/ml/processing/train'),\n",
    "                               ProcessingOutput(output_name='test_pca',\n",
    "                                                source='/opt/ml/processing/test')]\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_config = preprocessing_job_description['ProcessingOutputConfig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train_pca':\n",
    "        preprocessed_training_data = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'test_pca':\n",
    "        preprocessed_test_data = output['S3Output']['S3Uri']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(preprocessed_training_data + '/train_sparse_pca.csv', index_col='ID').to_csv(os.path.join(data_dir, 'train_sparse_pca.csv'), header=False, index=False)\n",
    "pd.read_csv(preprocessed_test_data + '/test_sparse_pca.csv', index_col='ID').to_csv(os.path.join(data_dir, 'test_sparse_pca.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.upload_data(path=os.path.join(data_dir, 'train_sparse_pca.csv'), bucket=bucket, key_prefix=data_prefix)\n",
    "sagemaker_session.upload_data(path=os.path.join(data_dir, 'test_sparse_pca.csv'), bucket=bucket, key_prefix=data_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_sparse = train_df_sparse.corr(method='spearman')[0][1:]\n",
    "\n",
    "#Plotting output correlation\n",
    "plt.scatter(range(len(correlation_sparse)), np.sort(correlation_sparse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
