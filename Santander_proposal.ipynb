{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Proposal\n",
    "Antonio Gonzalez\n",
    "June 29, 2020\n",
    "\n",
    "## Proposal\n",
    "\n",
    "### Domain Background\n",
    "This project is drawn from a Kaggle competition hosted by Santander in 2018. The competition's aim is to predict the financial value of a customer transaction based on unique attributes of the customer and their past interactions. The project was sponsored by Santander with the broader goal of more accurately anticipating and addressing customer needs in a personalized manner. \n",
    "\n",
    "Rising consumer expectations for personalized service, coupled with new market entrants that are equipped to cost effectively address those expectations, are some of the most fundamental competitive pressures now facing financial services. An industry that has traditionally relied on the personal touch in deepening relationships has had to rethink the meaning of personalization when many interactions are now digital touchpoints that do not afford the opportunity for human-based interaction. Consequently, many financial instutions are looking to invest heavily in predictive analytics. I decided to try my hand at this competition to better understand the dynamics of consumer needs, as well as the nature of investments firms are making.\n",
    "\n",
    "### Problem Statement\n",
    "The problem to be solved is predicting the financial value of transactions for each potential banking customer based on available customer characteristcs. A predictive model will be generated that will be able to estimate the value of customer transactions.\n",
    "\n",
    "### Datasets and Inputs\n",
    "The model will be built and evaluated based on anonymized customer data provided by Santander. Each row of the data represents an individual customer, and has an associated 4,992 columns (excluding ID). The target is a continuous variable of an unknown financial denomination, while the remaining 4,991 columns represent anonymized features. Consequently, domain knowledge of financial services will be unavailable in solving this problem. Moreover, the vast majority of feature values are zero, with a small number of large outliers, indicating that feature engineering will have to deal with significantly skewed distributions.\n",
    "\n",
    "### Solution Statement\n",
    "\n",
    "A solution to the problem will be a trained algorithm able to accurately predict transaction values for 49,342 Santander customers in the test dataset.\n",
    "\n",
    "### Benchmark \n",
    "Predictive outputs will be compared to actual financial data for customers in the test dataset. A good baseline score to exceed will be the evaluation metric score on a basic linear regression without any feature selection or transformation - a RMSLE of approximately 1.4.\n",
    "\n",
    "### Evaluation Metric\n",
    "The model's outputs will be evaluated against the actual results using Root Mean Squared Logarithmic Error, calculated as :\n",
    "\n",
    " $$ ϵ= \\sqrt{1n∑i=1n(log(pi+1)−log(ai+1))2} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "_ϵ_ is the RMSLE value (score) <br>\n",
    "_n_ is the total number of observations in the (public/private) data set, <br>\n",
    "$p_{i}$ is your prediction of target, and <br>\n",
    "$a_{i}$ is the actual target for _i_. <br>\n",
    "log(_x_) is the natural logarithm of _x_\n",
    "\n",
    "### Project Design\n",
    "I will begin by exploring out the data. Because there are more features than observations in the training data, and almost all of the feature distributions are heavily skewed left, I will be prioritizing the exploration of potential approaches to feature reduction and transformation. I will begin with simple feature selection based on target and cross-feature correlation, with appropriate transformations, and then move on to different approaches to PCA-based reduction as appropriate after testing several potential machine learning models. \n",
    "\n",
    "I will create a pipeline to run baseline predictive models using cross validation and evaluate based on their mean MSLE scores. initial models i will aim to use are:\n",
    "- Ridge Regression\n",
    "-Lasso Regression\n",
    "- SVR\n",
    "- Random Forest Regression\n",
    "- SGD Regression\n",
    "- Gradient Boosted Regression\n",
    "- Ada Boosted Regression\n",
    "\n",
    "If the results of any model appear viable, i will further refine that model through hyperparameter tuning for potential deployment with the test dataset. Otherwise, I will then return to the feature selection stage and attempt to further refine my approach to dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
